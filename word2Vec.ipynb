{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ari_time</th>\n",
       "      <th>ari_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>ANZ Bank Seeking to Double Its Northeast Asian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>Abu Dhabi Said to Invite Sulfur Granulation Pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>Agility, Arabtec, Gulf Finance House, Sahara, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>Air India, Boeing to Hold Talks Over Plane Del...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>Al-Qaeda Gunmen Storm Iraqi Central Bank, Kill...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ari_time                                          ari_title  price\n",
       "139  2010-06-13  ANZ Bank Seeking to Double Its Northeast Asian...      1\n",
       "145  2010-06-13  Abu Dhabi Said to Invite Sulfur Granulation Pl...      1\n",
       "47   2010-06-13  Agility, Arabtec, Gulf Finance House, Sahara, ...      1\n",
       "188  2010-06-13  Air India, Boeing to Hold Talks Over Plane Del...      1\n",
       "174  2010-06-13  Al-Qaeda Gunmen Storm Iraqi Central Bank, Kill...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_pickle(\"./train.pkl\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ari_time</th>\n",
       "      <th>ari_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>bank of new zealand say to start marketing nz$...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-06-14</td>\n",
       "      <td>banking regulator say forces driving china 's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-06-15</td>\n",
       "      <td>broad commits # % of wealth take buffett pledg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-06-16</td>\n",
       "      <td>u.s. stocks trim losses south korea tell north...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-06-17</td>\n",
       "      <td>australian prosecutors get evidence from monta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ari_time                                          ari_title  price\n",
       "0  2010-06-13  bank of new zealand say to start marketing nz$...      1\n",
       "1  2010-06-14  banking regulator say forces driving china 's ...      1\n",
       "2  2010-06-15  broad commits # % of wealth take buffett pledg...      0\n",
       "3  2010-06-16  u.s. stocks trim losses south korea tell north...      1\n",
       "4  2010-06-17  australian prosecutors get evidence from monta...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduce_df = pd.read_pickle(\"./train_reduce.pkl\")\n",
    "train_reduce_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ari_time'], ['ari_title'], ['price'], ['anz', 'bank', 'seeking', 'to', 'double', 'its', 'northeast', 'asian', 'revenue', 'australian', 'reports'], ['abu', 'dhabi', 'said', 'to', 'invite', 'sulfur', 'granulation', 'plant', 'bids', 'for', 'shah', 'gas', 'field'], ['agility', 'arabtec', 'gulf', 'finance', 'house', 'sahara', 'zain', 'gulf', 'equity', 'preview'], ['air', 'india', 'boeing', 'to', 'hold', 'talks', 'over', 'plane', 'delivery', 'press', 'trust', 'reports'], ['al', 'qaeda', 'gunmen', 'storm', 'iraqi', 'central', 'bank', 'kill', 'wound', 'army', 'says'], ['algeria', 'slovenia', 'tied', 'at', 'halftime', 'of', 'world', 'cup', 'group', 'game'], ['asian', 'junk', 'bonds', 'risk', 'company', 'calls', 'amid', 'biggest', 'debt', 'rally', 'this', 'decade']]\n",
      "CPU times: user 9.05 s, sys: 127 ms, total: 9.18 s\n",
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# del corpus\n",
    "corpus = list(map(simple_preprocess, train_reduce_df)) # ['ari_title']\n",
    "corpus += list(map(simple_preprocess, train_df['ari_title']))\n",
    "\n",
    "print(corpus[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 722 ms, total: 1min 27s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Word2Vec(corpus, size=EMBEDDING_DIM, iter=10)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word count is 17 among 312217 titles.\n"
     ]
    }
   ],
   "source": [
    "# [Optional] Calculate max length among all titles for checking, ref MAX_WORD_COUNT in constants.ipynb\n",
    "max_word_count = np.max(list(map(len, corpus)))\n",
    "print(\"Max word count is %d among %d titles.\" % (max_word_count, len(train_df['ari_title'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=22839, size=100, alpha=0.025)\n",
      "(312217, 3)\n",
      "2896872 61424 38585\n",
      "0.6281746548580359\n"
     ]
    }
   ],
   "source": [
    "# [Optional] Calculate number of unseen words for checking\n",
    "words = {}\n",
    "word_count = 0\n",
    "unseen_count = 0\n",
    "for wl in corpus:\n",
    "    for word in wl:\n",
    "        word_count += 1\n",
    "        if word in words:\n",
    "            continue\n",
    "        else:\n",
    "            words[word] = True\n",
    "        if word not in model.wv.vocab:\n",
    "            unseen_count += 1            \n",
    "non_repeated_words = len(words)\n",
    "\n",
    "print(model)\n",
    "print(train_df.shape)\n",
    "print(word_count, non_repeated_words, unseen_count)\n",
    "print(unseen_count / non_repeated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
