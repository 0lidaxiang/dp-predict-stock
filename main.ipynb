{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "from collections import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus, word2idx):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "        # new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training set\n",
    "train_df_sample = pd.read_pickle('train_reduce.pkl')\n",
    "train_texts = train_df_sample['ari_title']\n",
    "label_list = train_df_sample['price']\n",
    "\n",
    "# Read testing set\n",
    "test_pickle_df = pd.read_pickle('train_reduce.pkl')\n",
    "test_texts = test_pickle_df[\"ari_title\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "vocab_num = len(wvv.items()) + 1\n",
    "wvv_keys_list = list(wvv.keys())\n",
    "vocab_list = [(word, word_vectors[word]) for word, _ in wvv.items()]\n",
    "del word_vectors, wvv, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding matrix\n",
    "\n",
    "word_vec_len = 50\n",
    "embedding_matrix = np.zeros((vocab_num , word_vec_len))\n",
    "word2idx = {}\n",
    "\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate max length among all titles for checking\n",
    "max_title_length = np.max(list(map(len, train_texts)))\n",
    "print(\"Max title length is %d among %d titles.\" % (max_title_length, len(train_texts)))\n",
    "\n",
    "max_doc_word_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training inputs\n",
    "x_train_texts_idx = text_to_index(train_df['ari_title'], word2idx)\n",
    "X_train = pad_sequences(x_train_texts_idx, maxlen= max_doc_word_length)\n",
    "\n",
    "Y_train = label_list\n",
    "assert(len(X_train) == len(Y_train))\n",
    "data_length = len(X_train)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=50, weights=[embedding_matrix], \n",
    "                            input_length=max_doc_word_length, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(16))    \n",
    "\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Dense(20, activation='relu'))\n",
    "\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "history = model.fit(x=X_train, y=Y_train, \n",
    "                    batch_size=100,  epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('models/my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loss_history = np.array(history.history['loss'])\n",
    "np.savetxt(\"loss_history.txt\", np_loss_history, delimiter=\",\")\n",
    "loss_history = np.loadtxt(\"loss_history.txt\")\n",
    "\n",
    "np_acc_history = np.array(history.history['acc'])\n",
    "np.savetxt(\"acc_history.txt\", np_acc_history, delimiter=\",\")\n",
    "acc_history = np.loadtxt(\"acc_history.txt\")\n",
    "\n",
    "print(\"accuracy 準確度: \")\n",
    "fig = plt.figure(1)\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, num_epochs, acc_history.shape[0])\n",
    "plt.plot(x, acc_history, '-r');  # dotted red\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"loss 損失函數: \")\n",
    "fig = plt.figure(2)\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, num_epochs, loss_history.shape[0])\n",
    "plt.plot(x, loss_history, '-g');  # dotted red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss_accuracy = model.evaluate(X_train[0:100], Y_train[0:100], verbose=1)\n",
    "print(type(loss_accuracy), loss_accuracy)\n",
    "\n",
    "test_sequences1 = X_train[6377:6577]\n",
    "\n",
    "predict_res = model.predict(test_sequences1, batch_size= 1, verbose=0)\n",
    "\n",
    "# final_res = []\n",
    "# for pre_res in predict_res:\n",
    "#     final_res.append(pre_res)\n",
    "# print(predict_res[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格式化輸出結果成 1 和 0\n",
    "# 預測結果小於0.5的保存爲0,表示判斷爲會下跌。否則表示上漲或有限小數位的不變。\n",
    "final_res = []\n",
    "for value in predict_res:\n",
    "    final_res.append(int(round(value[0])))\n",
    "\n",
    "RIGHT_INDEX= 10\n",
    "for pv,fv in zip(predict_res[0:RIGHT_INDEX], final_res[0:RIGHT_INDEX]):\n",
    "    print( repr(pv[0]).ljust(15)[:4] , repr(fv).rjust(3)) \n",
    "\n",
    "# print(final_res[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和真實的股票指數的變化比較，輸出預測的準確率\n",
    "x1 = final_res[100:]\n",
    "x2 = []\n",
    "for val in Y_train[6477:]:\n",
    "    x2.append(val)\n",
    "\n",
    "acc_i = 0\n",
    "for v1,v2 in zip(x1, x2):\n",
    "    compare_res = (v1 == v2)\n",
    "#     print(compare_res)\n",
    "    if compare_res:\n",
    "        acc_i += 1\n",
    "print(\"預測準確率爲:\", acc_i / len(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ress = np.random.randint(2, size=10)\n",
    "acc_j = 0\n",
    "# print(pre_ress)\n",
    "for v1,v2 in zip(pre_ress, x2):\n",
    "    compare_res = (v1 == v2)\n",
    "#     print(compare_res)\n",
    "    if compare_res:\n",
    "        acc_j += 1\n",
    "print(\"random 1 準確率爲:\", acc_j / len(pre_ress))\n",
    "\n",
    "pre_ress = np.random.randint(2, size=10)\n",
    "acc_j = 0\n",
    "# print(pre_ress)\n",
    "for v1,v2 in zip(pre_ress, x2):\n",
    "    compare_res = (v1 == v2)\n",
    "#     print(compare_res)\n",
    "    if compare_res:\n",
    "        acc_j += 1\n",
    "print(\"random 2 準確率爲:\", acc_j / len(pre_ress))\n",
    "\n",
    "pre_ress = np.random.randint(2, size=10)\n",
    "acc_j = 0\n",
    "# print(pre_ress)\n",
    "for v1,v2 in zip(pre_ress, x2):\n",
    "    compare_res = (v1 == v2)\n",
    "#     print(compare_res)\n",
    "    if compare_res:\n",
    "        acc_j += 1\n",
    "print(\"random 3 準確率爲:\", acc_j / len(pre_ress))\n",
    "# print(type(x1), type(x2), x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "# print(len(final_res))\n",
    "# result_txt = \"local_result001\" + \".txt\"\n",
    "# ids = 0\n",
    "# with open(result_txt, 'w') as out:\n",
    "#     out.write(\"id,pre_price\" + '\\n')\n",
    "#     for value in final_res:\n",
    "#         out.write(str(ids) + \",\" + str(int (round(value[0]))) + \",\" + str(int (round(value[0]))) + '\\n')\n",
    "#         ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
