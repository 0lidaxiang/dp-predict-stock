{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb\n",
    "%run word2vec_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle and split training and testing sets\n",
    "df = pd.read_pickle('train_reduce.pkl')\n",
    "train_df, test_df = train_test_split(df, test_size=0.8, shuffle=False)\n",
    "train_df, test_df = test_df, train_df\n",
    "\n",
    "print('Rows of dataset=%d, training set rows=%s, testing set rows=%d' % (len(df), len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embedding matrix and word2idx dict\n",
    "embedding_matrix, word2idx = createEmbeddingMatrix(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training inputs\n",
    "x_texts_idx = text_to_index(train_df['ari_title'], word2idx)\n",
    "x_texts_idx_pad = pad_sequences(x_texts_idx, maxlen=MAX_WORD_COUNT)\n",
    "X_train = x_texts_idx_pad\n",
    "# X_train = np.insert(x_texts_idx_pad, 0, train_df['ari_time'].values, axis=1)\n",
    "\n",
    "Y_train = train_df['price']\n",
    "assert(len(X_train) == len(Y_train))\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "# input_dim=embedding_matrix.shape[0]\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=MAX_WORD_COUNT, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# model.add(LSTM(16))\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Dense(20, activation='relu'))\n",
    "\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# prev: optimizer=adam\n",
    "# rmsprop: default lr=0.001\n",
    "# rmsprop2 = keras.optimizers.RMSprop(lr=0.05)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_epoch = 0\n",
    "num_epochs = 200\n",
    "num_batch_size = 100  # 100\n",
    "\n",
    "# Load previous model\n",
    "# init_epoch = 98\n",
    "# model_path = 'models_intermediate/epoch%04d.h5' % init_epoch\n",
    "# print('Loadinig model', model_path)\n",
    "# model = load_model(model_path)\n",
    "\n",
    "# Model checkpoint\n",
    "filepath=\"models_intermediate/epoch{epoch:04d}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(x=X_train, y=Y_train, batch_size=num_batch_size, \n",
    "                    initial_epoch=init_epoch, epochs=num_epochs, \n",
    "                    callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('models/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss chart\n",
    "np_loss_history = np.array(history.history['loss'])\n",
    "np.savetxt(\"loss_history.txt\", np_loss_history, delimiter=\",\")\n",
    "loss_history = np.loadtxt(\"loss_history.txt\")\n",
    "\n",
    "np_acc_history = np.array(history.history['acc'])\n",
    "np.savetxt(\"acc_history.txt\", np_acc_history, delimiter=\",\")\n",
    "acc_history = np.loadtxt(\"acc_history.txt\")\n",
    "\n",
    "print(\"accuracy 準確度: \")\n",
    "fig = plt.figure(1)\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, num_epochs, acc_history.shape[0])\n",
    "plt.plot(x, acc_history, '-r');  # dotted red\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"loss 損失函數: \")\n",
    "fig = plt.figure(2)\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, num_epochs, loss_history.shape[0])\n",
    "plt.plot(x, loss_history, '-g');  # dotted red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model Evaluation #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation by training data\n",
    "n_first = len(X_train)\n",
    "loss_accuracy = model.evaluate(X_train[0:n_first], Y_train[0:n_first], verbose=1)\n",
    "print('loss=%.4f, accuracy=%.4f' % (loss_accuracy[0], loss_accuracy[1]))\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing inputs\n",
    "x_test_texts_idx = text_to_index(test_df['ari_title'], word2idx)\n",
    "X_test = pad_sequences(x_test_texts_idx, maxlen=MAX_WORD_COUNT)\n",
    "\n",
    "Y_test = test_df['price']\n",
    "assert(len(X_test) == len(Y_test))\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation by testing data\n",
    "n_first = len(X_test)\n",
    "loss_accuracy = model.evaluate(X_test[0:n_first], Y_test[0:n_first], verbose=1)\n",
    "print('loss=%.4f, accuracy=%.4f' % (loss_accuracy[0], loss_accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
