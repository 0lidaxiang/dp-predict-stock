{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 2 dict_keys(['Meta Data', 'Time Series (Daily)'])\n",
      "{'1. Information': 'Daily Time Series with Splits and Dividend Events', '4. Output Size': 'Full size', '3. Last Refreshed': '2018-06-14', '5. Time Zone': 'US/Eastern', '2. Symbol': 'S&P500 Index'} \n",
      "\n",
      "4642\n",
      "CPU times: user 49.7 ms, sys: 49.2 ms, total: 99 ms\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 读取2000年至2013年的每一天的 S&P 股票的资料,如果有股市开的话\n",
    "\n",
    "# url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=MSFT&outputsize=full&apikey=demo'\n",
    "# web_data = requests.get(url)\n",
    "\n",
    "API_KEY = \"25IEBEKH83S6MQOE\"\n",
    "page_num = 1\n",
    "page_size = 100\n",
    "url_params = {\"function\":\"TIME_SERIES_DAILY_ADJUSTED\", \"symbol\": \".INX\", \"outputsize\": \"full\", \"apikey\": API_KEY}\n",
    "everything_url=\"https://www.alphavantage.co/query?\"\n",
    "web_data = requests.get(everything_url, params=url_params)\n",
    "\n",
    "web_data.encoding = 'utf-8'\n",
    "web_data_text = web_data.text\n",
    "json_acceptable_string = web_data_text.replace(\"'\", \"\\\"\")\n",
    "web_data_dict = json.loads(json_acceptable_string)\n",
    "\n",
    "print(type(web_data_dict), len(web_data_dict), web_data_dict.keys())\n",
    "\n",
    "del web_data, web_data_text, json_acceptable_string\n",
    "\n",
    "meta_data_dict = web_data_dict[\"Meta Data\"]\n",
    "stock_data_dict = web_data_dict[\"Time Series (Daily)\"]\n",
    "\n",
    "del web_data_dict\n",
    "\n",
    "print(meta_data_dict, \"\\n\")\n",
    "print(len(stock_data_dict))\n",
    "\n",
    "# print(type(stock_data_dict), stock_data_dict.keys())\n",
    "\n",
    "# stock_time = list(stock_data_dict.keys())\n",
    "# print(type(stock_time), stock_time[:3])\n",
    "\n",
    "# stock_val = stock_data_dict.values()\n",
    "# term = stock_data_dict[\"2018-05-24\"]\n",
    "# print(type(term), term, term[\"4. close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'4. close': '1455.2200', '2. high': '1478.0000', '5. adjusted close': '1455.2200', '1. open': '1469.2500', '6. volume': '931800000', '8. split coefficient': '1.0000', '3. low': '1438.3600', '7. dividend amount': '0.0000'} 1455.2200\n",
      "4642 4642\n",
      "1 1455.2200\n",
      "0 1399.4200\n",
      "1 1402.1100\n",
      "1 1368.6000\n",
      "0 1257.6400\n"
     ]
    }
   ],
   "source": [
    "# 把获取到的股票data按日期排序,并按照今天比昨天是否增长设置1和0\n",
    "# 今天比昨天价格更低为0,否则为1(可能是涨,可能是维持不变)\n",
    "ostock_data_dict = collections.OrderedDict(sorted(stock_data_dict.items()))\n",
    "\n",
    "new_stocks = dict()\n",
    "new_stock_keys = list(ostock_data_dict.keys())\n",
    "print(type(new_stock_keys))\n",
    "old_key = new_stock_keys[0]\n",
    "for now_key in new_stock_keys:\n",
    "    if ostock_data_dict[now_key][\"4. close\"] < ostock_data_dict[old_key][\"4. close\"]:\n",
    "        new_stocks[now_key] = 0\n",
    "    else:\n",
    "        new_stocks[now_key] = 1\n",
    "    old_key = now_key\n",
    "    \n",
    "# Print data for checking\n",
    "print(stock_data_dict[\"2000-01-03\"], stock_data_dict[\"2000-01-03\"][\"4. close\"])\n",
    "print(len(new_stocks), len(ostock_data_dict))\n",
    "print(new_stocks[\"2000-01-03\"], stock_data_dict[\"2000-01-03\"][\"4. close\"]) # first day from API\n",
    "print(new_stocks[\"2000-01-04\"], stock_data_dict[\"2000-01-04\"][\"4. close\"])\n",
    "print(new_stocks[\"2000-01-05\"], stock_data_dict[\"2000-01-05\"][\"4. close\"])\n",
    "print(new_stocks[\"2006-10-20\"], stock_data_dict[\"2006-10-20\"][\"4. close\"]) # first day of news dataset\n",
    "print(new_stocks[\"2010-12-31\"], stock_data_dict[\"2010-12-31\"][\"4. close\"]) # last day of news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure news dataset has been extracted (follow instructions on README.md)\n",
    "TRAINING_PATH = '../data/news/'\n",
    "TESTING_PATH = '../data/news/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 895; first=2006-10-20, last=2010-12-31\n"
     ]
    }
   ],
   "source": [
    "old_categories = [dirname for dirname in os.listdir(TRAINING_PATH)]\n",
    "categories= sorted(old_categories)\n",
    "print(\"Number of categories: %d; first=%s, last=%s\" % (len(categories), categories[0], categories[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 557 ms, total: 3.15 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read News Headlines\n",
    "train_list = []\n",
    "\n",
    "for category in categories:\n",
    "    category_path = TRAINING_PATH + category + \"/\"\n",
    "    \n",
    "    for filename in os.listdir(category_path):\n",
    "        filepath = category_path + filename\n",
    "        # print(filepath)\n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            try:\n",
    "                words = file.read().strip().split('-- http://www.bloomberg.com')\n",
    "                if words[0] == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    abs_words = words[0].split(\"\\n\")\n",
    "                    if len(abs_words) > 2:\n",
    "                        # Remove '-- ' prefix of time and title\n",
    "                        ari_title = abs_words[0].replace('-- ', '')\n",
    "                        ari_time = abs_words[2].replace('-- ', '')\n",
    "                        if category in stock_data_dict.keys():\n",
    "                            price = new_stocks[category]\n",
    "                            train_list.append([ari_time, ari_title, price])\n",
    "            except ValueError:\n",
    "                print (ValueError)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data frames: (55483, 3)\n",
      "2006-10-20T20:16:16Z ::: Inco's Net Soars on Higher Metal Prices, Breakup Fee\n",
      "2000-01-03\n",
      "2006-10-23T11:51:36Z ::: EU Energy Chief Backs German Plan for Price Controls\n",
      "2000-01-04\n",
      "2006-10-23T20:00:29Z ::: Ex-Plant Worker Shuster Pleads Guilty in Trading Case\n",
      "2000-01-05\n",
      "2006-10-24T10:53:59Z ::: Russia, Ukraine End Dispute That Cut Gas Supplies\n",
      "2000-01-06\n",
      "2006-10-24T01:32:04Z ::: Jim Cramer: Bare Escentuals, Allergan, Medicis, Avon\n",
      "2000-01-07\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_list, columns=[\"ari_time\", \"ari_title\", \"price\"])\n",
    "print(\"Shape of training data frames:\", train_df.shape)\n",
    "\n",
    "# Print data for checking\n",
    "for i in range(5):\n",
    "    print(train_list[i][0], ':::', train_list[i][1])\n",
    "    print(list(ostock_data_dict.keys())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 ms, sys: 13.3 ms, total: 41.6 ms\n",
      "Wall time: 39.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df.to_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# Read saved pickle file for checking\n",
    "\n",
    "pickle_df = pd.read_pickle('train.pkl')\n",
    "# train_df.equals(pickle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55483, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    1\n",
      "29    1\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pickle_df.shape)\n",
    "print(type(pickle_df))\n",
    "print(pickle_df[:30][\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
